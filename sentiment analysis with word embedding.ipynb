{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**We import imdb dataset of 25000 reviews, then we select top 10000 words from dataset to work with<br>\n",
    "X = reviewes that are tokenized in form of indexes that are related to a token<br>\n",
    "Y = if review is negative or positive<br>\n",
    "we normalize each review to be no more than 256 words and if it is less we add 0 to end of the review<br>\n",
    "\n",
    "later we use embedding layer of keras library to vectorize each word (token) to a vector with length of 32<br>\n",
    "next we flatten this vector using flatten layer of keras and then feed it to neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to fit model with sequence of tokens with specific length\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Flatten\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000 # num_words: integer or None. Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of 25000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been \n",
    "# preprocessed, and each review is encoded as a sequence of word indexes (integers)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256 # represents length of the sequence of tokens | int. Maximum sequence length. Any longer sequence will be truncated.\n",
    "embedding_size = 32 # size of the vector that represents each word\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad  =  'post' #'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all sequences to the same size of 256, add zero to empty places\n",
    "X_train_pad = pad_sequences(X_train, maxlen=max_len, padding=pad, truncating=pad)\n",
    "X_test_pad = pad_sequences(X_test, maxlen=max_len, padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Embedding layer vs Word2Vec and Glove *****<br><br>\n",
    "\n",
    "Embeddings (in general, not only in Keras) are methods for learning vector representations of categorical data.<br>\n",
    "They are most commonly used for working with textual data. Word2vec and GloVe are two popular frameworks for learning word embeddings. What embeddings do, is they simply learn to map the one-hot encoded categorical variables to vectors<br> \n",
    "of floating point numbers of smaller dimensionality then the input vectors. For example, one-hot vector representing<br> \n",
    "a word from vocabulary of size 50 000 is mapped to real-valued vector of size 100. Then, the embeddings vector is used for whatever you want to use it as features.<br>\n",
    "\n",
    "The difference is how Word2vec is trained, as compared to the \"usual\" learned embeddings layers. Word2vec is trained to predict if word belongs to the context, given other words, e.g. to tell if \"milk\" is a likely word given the<br> \n",
    "\"The cat was drinking...\" sentence begging. By doing so, we expect Word2vec to learn something about the language,<br> \n",
    "as in the quote \"You shall know a word by the company it keeps\" by John Rupert Firth. Using the above example,<br> \n",
    "Word2vec learns that \"cat\" is something that is likely to appear together with \"milk\", but also with \"house\", or \"pet\", so it is somehow similar to \"dog\". As a consequence, embeddings created by Word2vec, or similar models, learn to represent words with similar meanings using similar vectors.<br>\n",
    "\n",
    "On another hand, with embeddings learned as a layer of a neural network, the network may be trained to predict<br> \n",
    "whatever you want. For example, you can train your network to predict sentiment of a text. In such case, the embeddings<br>\n",
    "would learn features that are relevant for this particular problem. As a side effect, they can learn also some general<br> \n",
    "things about the language, but the network is not optimized for such task. Using the \"cat\" example, embeddings trained<br> \n",
    "for sentiment analysis may learn that \"cat\" and \"dog\" are similar, because people often say nice things about their pets.<br>\n",
    "\n",
    "In practical terms, you can use the pretrained Word2vec embeddings as features of any neural network (or other<br> \n",
    "algorithm). They can give you advantage if your data is small, since the pretrained embeddings were trained on<br> \n",
    "large volumes of text. On another hand, there are examples showing that learning the embeddings from your data, optimized for a particular problem, may be more efficient (Qi et al, 2018).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# add the embedding layer\n",
    "# input_dim : number of words, output_dim = size of vector for each word\n",
    "# input_length : length of each sequence (input review), name = name of layer\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_len,\n",
    "                    name='layer_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "# we add a flatten layer because we need to transform data before feeding it to a hidden layer\n",
    "# before we had a 256x32 input (32 dense layer) but after flattening we have 8192x1 flat layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=250, activation='relu')) # add first hidden layer with relu and 250 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dropout(0.5)) # add regularizer to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have binary classification so we use sigmoid for last layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 256, 32)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               2048250   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,368,501\n",
      "Trainable params: 2,368,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary() # shows a visual representation of the rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 14s 566us/step - loss: 0.4912 - acc: 0.7359 - val_loss: 0.3293 - val_acc: 0.8567\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 16s 637us/step - loss: 0.1715 - acc: 0.9385 - val_loss: 0.3654 - val_acc: 0.8475\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 15s 589us/step - loss: 0.0369 - acc: 0.9916 - val_loss: 0.4768 - val_acc: 0.8432\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 14s 562us/step - loss: 0.0060 - acc: 0.9995 - val_loss: 0.5476 - val_acc: 0.8456\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 15s 594us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.5925 - val_acc: 0.8470\n",
      "CPU times: user 3min 27s, sys: 10.9 s, total: 3min 38s\n",
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f505fddcdd8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 130us/step\n",
      "CPU times: user 7.15 s, sys: 188 ms, total: 7.34 s\n",
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_ = model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925464714813232 0.84696\n"
     ]
    }
   ],
   "source": [
    "print(eval_[0], eval_[1]) # loss / accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
