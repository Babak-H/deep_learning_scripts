{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import keras # based on tensorflow\n",
    "from keras.models import Sequential # initializes neural network\n",
    "from keras.layers import Dense # for implementing ANN layers, it also takes care of weight initilization for us\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier # keras wrapper for k-fold-cross validation\n",
    "from sklearn.model_selection import cross_val_score # normal cross validation\n",
    "from sklearn.model_selection import GridSearchCV # cross validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "X = dataset.iloc[:, 3:13].values # RowNumber, CustomerId, Surname has no effect on weather customer is gonna leave the bank or not\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# encoding categorical data\n",
    "# first we change our categorical data (non-numbers) to numeric data\n",
    "labelencoder_X_1 = LabelEncoder() # country france/germany/spain\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder() # gender male/female\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "\n",
    "# then we change the country to one-hot encoding\n",
    "# not doing it for gender since its only two options 0/1\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# dummy variable trap, if both 2,3 are zero then it means country is 1\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/babak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "/home/babak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  if __name__ == '__main__':\n",
      "/home/babak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/babak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  del sys.path[0]\n",
      "/home/babak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#for hidden layers best activation function is usually Relu\n",
    "#for output layer it can be softmax or sigmoid\n",
    "classifier = Sequential()\n",
    "\n",
    "# add the input layer and first hidden layer\n",
    "# output_dim : how many neurons in hidden layer | init : initial weight distribution | input_dim : how many input nodes\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=11))\n",
    "#dropout regularization, dropout 0.1 of neurons from the layer\n",
    "classifier.add(Dropout(p=0.1))\n",
    "# adding second hidden layer\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "#dropout regularization, dropout 0.1 of neurons from the layer\n",
    "classifier.add(Dropout(p=0.1))\n",
    "#adding output layer\n",
    "# since output is 0/1 we have only one output node and therefore choose sigmoid activation, for more than that use softmax method\n",
    "classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the ANN, use stochastic  gradient descent with adam method, for loss calculation we choose binary_crossentropy (similar to gradient descent)\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train) # when we fit and transform with this data later it will only transform based on min/max values of this\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "#fitting (training) ANN model with training Set\n",
    "                                 #mini-batch\n",
    "classifier.fit(X_train, y_train, batch_size=10, nb_epoch=100)\n",
    "\n",
    "# predict\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) # true / false if statement\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "new_pred = classifier.predict(sc.transform(np.array([[0.0 ,0,600,1,40,3,60000,2,1,1,50000]])))\n",
    "new_pred = (new_pred > 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation of neural network(first we use it just figure bias_variance relation):\n",
    "\n",
    "# create classifier object and return it:\n",
    "def build_classifier():\n",
    "    #initialize Sequential classifier (other model is graph)\n",
    "    # for hidden layers best activation function is usually Relu\n",
    "    # for output layer it can be softmax or sigmoid\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # add the input layer and first hidden layer\n",
    "    # output_dim : how many neurons in hidden layer | init : initial weight distribution | input_dim : how many input nodes\n",
    "    classifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=11))\n",
    "    # dropout regularization, dropout 0.1 of neurons from the layer\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    # adding second hidden layer\n",
    "    classifier.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "    classifier.add(Dropout(p=0.1))\n",
    "    #adding output layer\n",
    "    # since output is 0/1 we have only one output node and therefore choose sigmoid activation, for more than that use softmax method\n",
    "    classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "    # compile the ANN, use stochastic  gradient descent with adam method, for loss calculation we choose binary_crossentropy (similar to gradient descent)\n",
    "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return classifier\n",
    "\n",
    "# here we create our classifer object\n",
    "classifier = KerasClassifier(build_fn=build_classifier, batch_size=10, epochs=100)\n",
    "# here we do the cross validation, cv= number of folds, n_jobs=-1  : use all cpu cores for this task\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv=10, n_jobs=1)\n",
    "print(accuracies)\n",
    "\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()  # if we have high variance between different K-fold sets its a sign of overfitting in our training set\n",
    "print(mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning:\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=11))\n",
    "    classifier.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "    classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))\n",
    "    classifier.compile(optimizer= optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "# parameters than we want to tune and the values to try\n",
    "parameters = {'batch_size' : [25, 32], 'epochs' : [100, 500], 'optimizer' : ['adam', 'rmsprop']}\n",
    "\n",
    "# create the cross validation object and train it with 10 different folds\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=parameters, scoring='accuracy', cv=5)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# find best results\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)\n",
    "print(best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
