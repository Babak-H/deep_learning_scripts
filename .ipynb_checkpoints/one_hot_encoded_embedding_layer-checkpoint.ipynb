{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Word Embedding layers in Keras***<br>\n",
    "Word embeddings provide a dense representation of words and their relative meanings.\n",
    "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "Word embeddings can be learned from text data and reused among projects. They can also be learned \n",
    "as part of fitting a neural network on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 24], [11, 43], [17, 30], [4, 43], [32], [26], [34, 30], [39, 11], [34, 43], [24, 28, 24, 4]]\n",
      "[[ 6 24  0  0]\n",
      " [11 43  0  0]\n",
      " [17 30  0  0]\n",
      " [ 4 43  0  0]\n",
      " [32  0  0  0]\n",
      " [26  0  0  0]\n",
      " [34 30  0  0]\n",
      " [39 11  0  0]\n",
      " [34 43  0  0]\n",
      " [24 28 24  4]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!','Good work','Great effort','nice work','Excellent!','Weak','Poor effort!',\n",
    "'not good','poor work','Could have done better.']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# integer encode the document\n",
    "# we can integer encode each document. This means that as input the Embedding layer will have \n",
    "# sequences of integers. We could experiment with other more sophisticated bag of word model \n",
    "# encoding like counts or TF-IDF.\n",
    "\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "\n",
    "# The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to \n",
    "# have the same length.\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "\n",
    "# The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding \n",
    "# space of 8 dimensions.\n",
    "# the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word.\n",
    "# We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
