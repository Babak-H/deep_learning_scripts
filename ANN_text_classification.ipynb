{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982 2246 45\n"
     ]
    }
   ],
   "source": [
    "                                 # this shows how many classes of news reports we have in this dataset\n",
    "print(len(x_train), len(x_test), max(y_train))\n",
    "num_classes = max(y_train)+1 # the numbering for the classes begins from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for k,v in word_index.items():\n",
    "    index_to_word[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([index_to_word[x] for x in x_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** why do we use tokenizer.sequences_to_matrix() function? ***\n",
    "\n",
    "\n",
    "So we have sequences as the original format of the dataset, and typically we add padding to make all the sequences<br>\n",
    "of the same length. The reason why we are using sequences_to_matrix here though is that we are trying to transform<br> \n",
    "the data into a particular encoding. We don't want a sequence of indices of words that were in the sentence, but<br> \n",
    "rather we want a matrix to represent a sequence.<br>\n",
    "\n",
    "To kind've explain more visually, if we have the sentence \"the cat is black\" and that is represented by the integer<br> \n",
    "sequence [1 4 2 3], then we might want to encode that sequence into a matrix for better learning. We might do this<br> \n",
    "(like in the video) by using the binary format. That would create a 4x5 matrix that would look<br> \n",
    "like this [[0 1 0 0 0], [0 0 0 0 1], [0 0 1 0 0], [0 0 0 1 0]] where the 1 denotes if a word is present in<br> \n",
    "the sentence (this is an example where our only vocabulary is these 4 words; if it was more, then we would have<br> \n",
    "a much larger matrix with a lot more zeros in it, if that makes sense). \n",
    "\n",
    "Really, we using this convert sequences to matrices functionality to take these dense vectors and transform them<br>  into some more sparse, encoded matrix that has some representation that makes sense in a NLP context. Binary<br>  represents if a words is present, count counts the number of time a word appears, TF-IDF uses a specially<br>  statistical scoring method across sequences to weight words by the term frequency multiplied by their inverse<br>  document frequency, etc<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# we text this function that take top 10000 words, then turn each example (training[i]) into a one-hot vector \n",
    "# and if the words in article are included in the 10000 then give 1 , otherwise give 0\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for the classes\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "(8982, 46)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#   When input data is one-dimensional, such as the MLP, the shape \n",
    "#   must explicitly leave room for the shape of the mini-batch size \n",
    "#   used when splitting the data when training the network. Hence, \n",
    "#   the shape tuple is always defined with a hanging last dimension.\n",
    "#   For instance, \"(2,)\", as in the example below:\n",
    "model.add(Dense(512, input_shape=(max_words, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               5120512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                23598     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 5,144,110\n",
      "Trainable params: 5,144,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/2\n",
      "8083/8083 [==============================] - 32s 4ms/step - loss: 1.3078 - acc: 0.7176 - val_loss: 0.9705 - val_acc: 0.7931\n",
      "Epoch 2/2\n",
      "8083/8083 [==============================] - 29s 4ms/step - loss: 0.5151 - acc: 0.8852 - val_loss: 0.8864 - val_acc: 0.8120\n",
      "2246/2246 [==============================] - 1s 349us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "# verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8414560103140446 0.8049866429472862\n"
     ]
    }
   ],
   "source": [
    "print(score[0], score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "# this way if a word occurs more than one time in the text its count will increase in our one-hot vector\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='count')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(max(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/babak/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/2\n",
      "8083/8083 [==============================] - 27s 3ms/step - loss: 1.3139 - acc: 0.7326 - val_loss: 0.9844 - val_acc: 0.8053\n",
      "Epoch 2/2\n",
      "8083/8083 [==============================] - 26s 3ms/step - loss: 0.5673 - acc: 0.8789 - val_loss: 0.9024 - val_acc: 0.8187\n",
      "2246/2246 [==============================] - 1s 226us/step\n",
      "0.8778713806760385 0.8103294746480876\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(score[0], score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 10000)\n",
      "[0.         0.69309152 0.         ... 0.         0.         0.        ]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "6.214608098422191\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "# need to fit tokenizer on train data before applying tfidf to it.\n",
    "tokenizer.fit_on_sequences(x_train)\n",
    "\n",
    "## TF-IDF : TF(w) * IDF(w)     \n",
    "#  tf(w) = (Number of times the word appears in a document) / (Total number of words in the document)\n",
    "#  idf(w) = log(Number of documents / Number of documents that contain word w )\n",
    "# this way unique words get more attension than words that appear all the time\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(max(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/2\n",
      "8083/8083 [==============================] - 28s 3ms/step - loss: 1.2535 - acc: 0.7547 - val_loss: 0.9716 - val_acc: 0.8165\n",
      "Epoch 2/2\n",
      "8083/8083 [==============================] - 27s 3ms/step - loss: 0.4530 - acc: 0.9135 - val_loss: 1.0465 - val_acc: 0.8142\n",
      "2246/2246 [==============================] - 1s 238us/step\n",
      "1.0234859694568366 0.8018699911218187\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print(score[0], score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
